{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER SHEET"
      ],
      "metadata": {
        "id": "t4x7hJMXer01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is a parameter?\n"
      ],
      "metadata": {
        "id": "ZSvvojpue3G_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A parameter is a variable that is used to define a specific value within a function or an algorithm. It allows you to pass information to a function, enabling it to perform tasks in a dynamic and flexible manner."
      ],
      "metadata": {
        "id": "IOlaU_oYfYoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What is correlation?\n",
        " What does negative correlation mean?"
      ],
      "metadata": {
        "id": "f06Pm9yofZgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the degree to which two variables move in relation to each other. It is often represented by a correlation coefficient, which ranges from -1 to 1.\n",
        "Negative correlation: When one variable increases, the other decreases."
      ],
      "metadata": {
        "id": "du0aqPokfdyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Define Machine Learning. What are the main components in Machine Learning?\n"
      ],
      "metadata": {
        "id": "pgToA-kCfrEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It involves algorithms that can identify patterns and make decisions based on data. The goal is to create models that can predict outcomes, recognize patterns, and make data-driven decisions.\n",
        "\n",
        "Main Components of Machine Learning\n",
        "Data: The foundation of any ML model. Data can be structured or unstructured and is used to train the model.\n",
        "\n",
        "Features: Specific attributes or properties of the data that are used by the model for prediction. Feature selection is crucial for model accuracy.\n",
        "\n",
        "Model: The mathematical representation of the relationship between the input data and the output prediction. Examples include linear regression, decision trees, and neural networks.\n",
        "\n",
        "Algorithm: The method used to train the model by finding patterns in the data. Algorithms include supervised learning, unsupervised learning, and reinforcement learning.\n",
        "\n",
        "Training: The process of teaching the model using historical data and adjusting its parameters to minimize errors.\n",
        "\n",
        "Evaluation: Assessing the model's performance using metrics such as accuracy, precision, recall, and F1-score. This ensures the model's predictions are reliable.\n",
        "\n",
        "Hyperparameters: These are the configurations set before training the model, such as learning rate, number of layers in a neural network, or the number of trees in a random forest. Tuning these hyperparameters can significantly impact model performance.\n",
        "\n",
        "Deployment: Integrating the trained model into a real-world application to make predictions on new data."
      ],
      "metadata": {
        "id": "Pk6Aq1mGfwkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  How does loss value help in determining whether the model is good or not?\n"
      ],
      "metadata": {
        "id": "3-TegiXif7iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss value, also known as loss function or cost function, is a crucial metric in machine learning that quantifies how well the model's predictions match the actual target values. It essentially measures the error or difference between the predicted outputs and the true outputs. Here's how it helps in determining the quality of the model:\n",
        "\n",
        "Training Process: During the training phase, the model iteratively adjusts its parameters to minimize the loss value. A lower loss value indicates that the model's predictions are closer to the actual target values, suggesting better performance.\n",
        "\n",
        "Performance Indicator: By monitoring the loss value, you can gauge how well the model is learning from the data. If the loss value decreases consistently during training, it indicates that the model is improving. Conversely, if the loss value stagnates or increases, it suggests issues like underfitting, overfitting, or improper model configuration.\n",
        "\n",
        "Model Comparison: Loss value allows you to compare different models or algorithms. By evaluating the loss values of various models, you can determine which model provides the best performance for your specific task.\n",
        "\n",
        "Hyperparameter Tuning: Adjusting hyperparameters can significantly impact model performance. The loss value helps in fine-tuning these hyperparameters by providing feedback on how changes affect the model's accuracy."
      ],
      "metadata": {
        "id": "oLb-KuzfgAr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What are continuous and categorical variables?\n"
      ],
      "metadata": {
        "id": "Dm-KG59tgTGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous variables are those that can take on an infinite number of values within a given range. They are typically measured and can be divided into smaller and smaller increments. Examples of continuous variables include:\n",
        "\n",
        "Height (e.g., 175.5 cm)\n",
        "\n",
        "Weight (e.g., 68.2 kg)\n",
        "\n",
        "Temperature (e.g., 36.7°C)\n",
        "\n",
        "Time (e.g., 12.45 seconds)\n",
        "\n",
        "Continuous variables are often used in statistical analysis and can be visualized using histograms, line charts, or scatter plots.\n",
        "\n",
        "Categorical Variables\n",
        "Categorical variables, also known as qualitative variables, represent distinct categories or groups. These variables can take on a limited, fixed number of values, and they are often used to label or classify data. Examples of categorical variables include:\n",
        "\n",
        "Gender (e.g., Male, Female, Other)\n",
        "\n",
        "Marital Status (e.g., Single, Married, Divorced)\n",
        "\n",
        "Eye Color (e.g., Blue, Brown, Green)\n",
        "\n",
        "Product Category (e.g., Electronics, Clothing, Groceries)\n",
        "\n",
        "Categorical variables can be further divided into two types:\n",
        "\n",
        "Nominal Variables: Categories without a specific order (e.g., eye color).\n",
        "\n",
        "Ordinal Variables: Categories with a specific order or ranking (e.g., education level: High School, Bachelor's, Master's, PhD).\n",
        "\n",
        "Categorical variables are often visualized using bar charts or pie charts."
      ],
      "metadata": {
        "id": "UmorlpOlgYDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "Tj4WNSFlgfBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling categorical variables in machine learning is crucial, as most algorithms require numerical input. Here are some common techniques to transform categorical data into numerical format:\n",
        "\n",
        "1. One-Hot Encoding\n",
        "This technique converts categorical variables into a series of binary (0 or 1) columns. Each category is represented by a separate column.\n",
        "\n",
        "2. Label Encoding\n",
        "This technique assigns a unique numerical value to each category. For example, \"Red\" might be encoded as 0, \"Blue\" as 1, and \"Green\" as 2. However, this method introduces an ordinal relationship, which might not be appropriate for all categorical data.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Similar to label encoding, ordinal encoding is used when there is a meaningful order or ranking among the categories. For example, \"Low,\" \"Medium,\" and \"High\" could be encoded as 0, 1, and 2, respectively.\n",
        "\n",
        "4. Frequency Encoding\n",
        "This method assigns the frequency of each category as the value.\n",
        "\n",
        "5. Target Encoding\n",
        "Target encoding replaces categories with the mean of the target variable for each category. This method can be useful for categorical variables with high cardinality.\n",
        "\n"
      ],
      "metadata": {
        "id": "sEpzxjGLgo9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  What do you mean by training and testing a dataset?\n"
      ],
      "metadata": {
        "id": "wcaIgE2phBZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, the process of building and evaluating a model involves dividing the dataset into two main parts: the training set and the testing set. This helps ensure the model generalizes well to new, unseen data. Here's a brief overview of each:\n",
        "\n",
        "Training Dataset\n",
        "Purpose: Used to train the machine learning model.\n",
        "\n",
        "Contents: Contains a large portion of the original dataset, typically around 70-80%.\n",
        "\n",
        "Process: The model learns patterns and relationships from this data by adjusting its parameters to minimize errors.\n",
        "\n",
        "Goal: To enable the model to make accurate predictions based on the input data.\n",
        "\n",
        "Testing Dataset\n",
        "Purpose: Used to evaluate the performance of the trained model.\n",
        "\n",
        "Contents: Contains the remaining portion of the original dataset, typically around 20-30%.\n",
        "\n",
        "Process: The model makes predictions on this data, and these predictions are compared to the actual target values.\n",
        "\n",
        "Goal: To assess how well the model generalizes to new, unseen data and to detect issues like overfitting or underfitting."
      ],
      "metadata": {
        "id": "6_p_3NHzhJeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is sklearn.preprocessing?\n"
      ],
      "metadata": {
        "id": "6yyfi8m2hTVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in the popular machine learning library scikit-learn. It provides various functions and classes to preprocess and transform data, making it suitable for machine learning algorithms. Preprocessing is a crucial step in the data preparation process, as it helps improve the quality and performance of the models."
      ],
      "metadata": {
        "id": "TcDbUpQZhaOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is a Test set?\n"
      ],
      "metadata": {
        "id": "YnvDCOMFhhsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is a subset of your dataset that is used to evaluate the performance of a trained machine learning model. It serves as a crucial component in the model development process, ensuring that the model can generalize well to new, unseen data."
      ],
      "metadata": {
        "id": "_97asufThlJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        " How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "Jk__bY4hhwn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can use the train_test_split function from the sklearn.model_selection module to split your dataset into training and testing sets.\n",
        "\n",
        "Approach to a Machine Learning Problem\n",
        "Define the Problem:\n",
        "\n",
        "Understand the problem you're trying to solve.\n",
        "\n",
        "Define the goals and objectives clearly.\n",
        "\n",
        "Collect Data:\n",
        "\n",
        "Gather relevant data from various sources.\n",
        "\n",
        "Ensure data quality and completeness.\n",
        "\n",
        "Explore and Analyze Data:\n",
        "\n",
        "Perform exploratory data analysis (EDA) to understand data distribution and patterns.\n",
        "\n",
        "Identify relationships, correlations, and outliers.\n",
        "\n",
        "Preprocess Data:\n",
        "\n",
        "Clean and preprocess the data (e.g., handling missing values, encoding categorical variables, scaling).\n",
        "\n",
        "Split the data into training and testing sets.\n",
        "\n",
        "Select a Model:\n",
        "\n",
        "Choose appropriate machine learning algorithms based on the problem type (e.g., regression, classification).\n",
        "\n",
        "Consider multiple models to find the best fit.\n",
        "\n",
        "Train the Model:\n",
        "\n",
        "Train the selected model(s) using the training data.\n",
        "\n",
        "Fine-tune hyperparameters to optimize performance.\n",
        "\n",
        "Evaluate the Model:\n",
        "\n",
        "Evaluate the model's performance using the testing data.\n",
        "\n",
        "Use metrics like accuracy, precision, recall, F1-score, or mean squared error.\n",
        "\n",
        "Improve the Model:\n",
        "\n",
        "Identify areas for improvement (e.g., feature engineering, more data, different algorithms).\n",
        "\n",
        "Iterate through the process to enhance the model's performance.\n",
        "\n",
        "Deploy the Model:\n",
        "\n",
        "Integrate the trained model into a real-world application.\n",
        "\n",
        "Monitor the model's performance and make necessary adjustments.\n",
        "\n",
        "Documentation and Reporting:\n",
        "\n",
        "Document the entire process, including assumptions, decisions, and results.\n",
        "\n",
        "Communicate findings and insights effectively."
      ],
      "metadata": {
        "id": "jqbQvPYGh2aW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "MGsCGxc5iKU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) is a crucial step before fitting a model to the data because it helps you understand the dataset's structure, patterns, and underlying relationships. Here are the key reasons why EDA is important:\n",
        "\n",
        "Data Quality Assessment:\n",
        "\n",
        "Identify missing values, outliers, and inconsistencies.\n",
        "\n",
        "Ensure data completeness and accuracy.\n",
        "\n",
        "Understanding Data Distribution:\n",
        "\n",
        "Visualize the distribution of features to identify skewness, kurtosis, and other statistical properties.\n",
        "\n",
        "Detect anomalies or unusual patterns.\n",
        "\n",
        "Feature Relationships:\n",
        "\n",
        "Examine correlations and interactions between features.\n",
        "\n",
        "Identify important features that might have a significant impact on the target variable.\n",
        "\n",
        "Data Transformation Needs:\n",
        "\n",
        "Determine the need for scaling, normalization, or encoding of features.\n",
        "\n",
        "Identify the appropriate techniques for transforming categorical variables.\n",
        "\n",
        "Hypothesis Generation:\n",
        "\n",
        "Formulate hypotheses based on observed patterns and relationships.\n",
        "\n",
        "Guide the selection of machine learning algorithms and models.\n",
        "\n",
        "Model Selection and Validation:\n",
        "\n",
        "Choose the appropriate models and validation techniques based on data characteristics.\n",
        "\n",
        "Avoid potential pitfalls such as overfitting or underfitting.\n",
        "\n",
        "Feature Engineering:\n",
        "\n",
        "Create new features or modify existing ones based on insights gained from EDA.\n",
        "\n",
        "Improve model performance by incorporating meaningful features.\n",
        "\n",
        "Visual Insights:\n",
        "\n",
        "Use visualizations (e.g., histograms, scatter plots, box plots) to communicate data insights effectively.\n",
        "\n",
        "Gain a deeper understanding of data trends and patterns."
      ],
      "metadata": {
        "id": "5srczkAiiP6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  What is correlation?\n"
      ],
      "metadata": {
        "id": "0JB3PMRliZ91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the extent to which two variables are related to each other. It quantifies the strength and direction of the relationship between the variables."
      ],
      "metadata": {
        "id": "GF6LN8k0ilJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  What does negative correlation mean?"
      ],
      "metadata": {
        "id": "Zi6-nLbhil3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative correlation means that as one variable increases, the other variable tends to decrease."
      ],
      "metadata": {
        "id": "s0wa7RkMiw_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  How can you find correlation between variables in Python?\n"
      ],
      "metadata": {
        "id": "Yj-GPwJbi1tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can find the correlation between variables using various methods. Here are some common techniques:\n",
        "\n",
        "1. Using Pandas\n",
        "The pandas library provides a convenient method to calculate the correlation matrix.\n",
        "\n",
        "2. Using NumPy\n",
        "The numpy library provides a method to calculate the Pearson correlation coefficient.\n",
        "\n",
        "3. Using Scipy\n",
        "The scipy library provides functions to calculate different types of correlation coefficients, such as Pearson, Spearman, and Kendall.\n",
        "\n"
      ],
      "metadata": {
        "id": "y9rpGA6ojAS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  What is causation? Explain difference between correlation and causation with an example.\n"
      ],
      "metadata": {
        "id": "WaP4qRBbjTUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation, or causality, indicates that one event or variable directly influences another. In other words, a change in one variable is responsible for a change in another variable. It implies a cause-and-effect relationship between the variables.\n",
        "\n",
        "Difference Between Correlation and Causation\n",
        "Correlation:\n",
        "\n",
        "Describes the relationship between two variables.\n",
        "\n",
        "Indicates how closely the variables move together.\n",
        "\n",
        "Does not imply that one variable causes the change in the other.\n",
        "\n",
        "Causation:\n",
        "\n",
        "Indicates that one variable directly affects another.\n",
        "\n",
        "Establishes a cause-and-effect relationship.\n",
        "\n",
        "Implies that changes in one variable result in changes in the other."
      ],
      "metadata": {
        "id": "VkvpGV-fjYE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What is an Optimizer? What are different types of optimizers? Explain each with an example.\n"
      ],
      "metadata": {
        "id": "DyLnDPXYjkX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer in machine learning is an algorithm or method used to adjust the weights and biases of a model during training to minimize the loss function. It helps the model learn by updating parameters based on the gradients calculated from the loss function.\n",
        "\n",
        "Types of Optimizers\n",
        "Here are some common types of optimizers, each with a brief explanation and example:\n",
        "\n",
        "Gradient Descent\n",
        "\n",
        "Explanation: The most basic optimization algorithm. It iteratively updates the model parameters by moving in the direction of the negative gradient of the loss function.\n",
        "\n",
        "Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Explanation: A variation of gradient descent where the model parameters are updated for each training example rather than the entire dataset. It introduces randomness and helps in faster convergence.\n",
        "\n",
        "Mini-Batch Gradient Descent\n",
        "\n",
        "Explanation: A compromise between batch gradient descent and SGD. It updates model parameters using a small random subset (mini-batch) of the training data, combining the benefits of both methods.\n",
        "\n",
        "Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Explanation: An advanced optimizer that combines the ideas of momentum and RMSprop. It adapts the learning rate for each parameter by maintaining a running average of both the gradients and their squared values.\n",
        "\n",
        "RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "Explanation: An optimizer that adapts the learning rate for each parameter by dividing the gradient by a running average of its recent magnitudes. It helps in smoothing out the steps and accelerating convergence."
      ],
      "metadata": {
        "id": "4lBiQswajo8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "UKNxG5x0j7EY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module within the scikit-learn library in Python that provides various linear models for regression and classification tasks. Linear models assume a linear relationship between the input variables (features) and the target variable."
      ],
      "metadata": {
        "id": "Oxx57hyVj-2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.  What does model.fit() do? What arguments must be given?\n"
      ],
      "metadata": {
        "id": "mR29iuKwkdOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.fit() function in machine learning is used to train a model on a given dataset. It fits the model to the training data by adjusting the model parameters to minimize the loss function.\n",
        "\n",
        "Arguments Required for model.fit()\n",
        "The main arguments required for model.fit() are:\n",
        "\n",
        "X (features): The input data used to train the model. It can be an array or DataFrame of shape (n_samples, n_features).\n",
        "\n",
        "y (target): The target values corresponding to the input data. It can be an array or Series of shape (n_samples,)."
      ],
      "metadata": {
        "id": "oEISuD20kmDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "bSHW-DPrkqIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.predict() function in machine learning is used to make predictions on new data based on the model that has been trained. After fitting the model to the training data using model.fit(), you use model.predict() to generate predicted values for the input features provided.\n",
        "\n",
        "Arguments Required for model.predict()\n",
        "The main argument required for model.predict() is:\n",
        "\n",
        "X (features): The input data on which predictions are to be made. It should have the same number of features as the data used for training."
      ],
      "metadata": {
        "id": "ZihhYukJkvO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "RRcIsZ-fk8g-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous Variables\n",
        "Continuous variables are those that can take on an infinite number of values within a given range. They are typically measured and can be divided into smaller and smaller increments. Examples of continuous variables include:\n",
        "\n",
        "Height (e.g., 175.5 cm)\n",
        "\n",
        "Weight (e.g., 68.2 kg)\n",
        "\n",
        "Temperature (e.g., 36.7°C)\n",
        "\n",
        "Time (e.g., 12.45 seconds)\n",
        "\n",
        "Continuous variables are often used in statistical analysis and can be visualized using histograms, line charts, or scatter plots.\n",
        "\n",
        "Categorical Variables\n",
        "Categorical variables, also known as qualitative variables, represent distinct categories or groups. These variables can take on a limited, fixed number of values, and they are often used to label or classify data. Examples of categorical variables include:\n",
        "\n",
        "Gender (e.g., Male, Female, Other)\n",
        "\n",
        "Marital Status (e.g., Single, Married, Divorced)\n",
        "\n",
        "Eye Color (e.g., Blue, Brown, Green)\n",
        "\n",
        "Product Category (e.g., Electronics, Clothing, Groceries)\n",
        "\n",
        "Categorical variables can be further divided into two types:\n",
        "\n",
        "Nominal Variables: Categories without a specific order (e.g., eye color).\n",
        "\n",
        "Ordinal Variables: Categories with a specific order or ranking (e.g., education level: High School, Bachelor's, Master's, PhD).\n",
        "\n",
        "Categorical variables are often visualized using bar charts or pie charts."
      ],
      "metadata": {
        "id": "pTep1C9mlENA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  What is feature scaling? How does it help in Machine Learning?\n"
      ],
      "metadata": {
        "id": "oTQQHwlflLlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is a technique used to normalize the range of independent variables or features in a dataset. It ensures that all features contribute equally to the model's performance by transforming them to a common scale.\n",
        "\n",
        "Importance of Feature Scaling in Machine Learning\n",
        "Improves Model Performance:\n",
        "\n",
        "Algorithms like gradient descent converge faster with scaled data.\n",
        "\n",
        "Ensures that features with larger ranges do not dominate the learning process.\n",
        "\n",
        "Enhances Accuracy:\n",
        "\n",
        "Models like k-nearest neighbors (KNN) and support vector machines (SVM) are sensitive to feature scales.\n",
        "\n",
        "Feature scaling improves the accuracy of distance-based algorithms.\n",
        "\n",
        "Prevents Numerical Instability:\n",
        "\n",
        "Helps in avoiding numerical overflow or underflow during calculations.\n",
        "\n",
        "Ensures stability and reliability of the model.\n",
        "\n",
        "Uniform Contribution:\n",
        "\n",
        "Ensures that all features contribute equally to the model training.\n",
        "\n",
        "Prevents features with larger magnitudes from overshadowing others."
      ],
      "metadata": {
        "id": "F3Scrtv4lRY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  How do we perform scaling in Python?\n"
      ],
      "metadata": {
        "id": "0UUPlUkpliV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can perform feature scaling using the sklearn.preprocessing module from the scikit-learn library."
      ],
      "metadata": {
        "id": "P7xYfxF8ll1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  What is sklearn.preprocessing?\n"
      ],
      "metadata": {
        "id": "0ldGPzFkluBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in the popular machine learning library scikit-learn. It provides various functions and classes to preprocess and transform data, making it suitable for machine learning algorithms. Preprocessing is a crucial step in the data preparation process, as it helps improve the quality and performance of the models."
      ],
      "metadata": {
        "id": "SEizqRsqlxgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "bKkl6-Ayl3te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can easily split your dataset into training and testing sets using the train_test_split function from the sklearn.model_selection module."
      ],
      "metadata": {
        "id": "dWl6RT3Ll7lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.  Explain data encoding?\n"
      ],
      "metadata": {
        "id": "9NMc2q7EmG-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical variables into a numerical format that machine learning algorithms can process. Many machine learning models require numerical input, so encoding categorical variables is a crucial step in the data preprocessing pipeline."
      ],
      "metadata": {
        "id": "tzp-TBKImKI7"
      }
    }
  ]
}